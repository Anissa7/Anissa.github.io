{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anissa7/Anissa.github.io/blob/main/Web_scapping_assignment_Group_8_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 0"
      ],
      "metadata": {
        "id": "dcO8Gj4g2pOe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebBEH1oyrakZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4  import BeautifulSoup\n",
        "import requests\n",
        "import os\n",
        "from PIL import Image\n",
        "from io import BytesIO\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def availableShips(passengerCount):\n",
        "    \"\"\"\n",
        "    Method that returns the list of ships\n",
        "    that can hold a given number of passengers.\n",
        "\n",
        "    Args:\n",
        "        passengerCount (int): number of passengers\n",
        "        to be transported\n",
        "\n",
        "    Returns:\n",
        "        list: List of ship names that can hold the given number of passengers\n",
        "    \"\"\"\n",
        "    url_link = 'https://swapi-api.alx-tools.com/api/starships/'\n",
        "    list_ships = []\n",
        "\n",
        "    while url_link:\n",
        "        response = requests.get(url_link)\n",
        "\n",
        "        # Handle error incase request fails\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        for ship in data['results']:\n",
        "            # Skip ships with invalid passenger values\n",
        "            if ship[\"passengers\"] not in [\"n/a\", \"unknown\", \"0\", \"none\"]:\n",
        "                try:\n",
        "                    ship[\"passengers\"] = ship[\"passengers\"].replace(\",\", \"\")\n",
        "                    if int(ship['passengers']) >= passengerCount:\n",
        "                        list_ships.append(ship['name'])\n",
        "                except ValueError:\n",
        "                    # Handle unexpected non-numeric passenger values\n",
        "                    pass\n",
        "\n",
        "        # Move to the next page if available\n",
        "        url_link = data['next']\n",
        "\n",
        "    return list_ships\n",
        "\n",
        "# Check for an example of a ship with 300 passengers\n",
        "\n",
        "ship=availableShips(300)\n",
        "print(ship)\n"
      ],
      "metadata": {
        "id": "fpqJ9R9P2s5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b2a78a1-6983-4391-e8e1-db8ff2950b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CR90 corvette', 'Death Star', 'Executor', 'Calamari Cruiser', 'Droid control ship', 'AA-9 Coruscant freighter', 'Republic Assault ship', 'Trade Federation cruiser', 'Republic attack cruiser']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        " Web Scraping- Scrap this site\n",
        " Scraping the tabular data to CSV file"
      ],
      "metadata": {
        "id": "faoT854U1rO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import nessecary libraries"
      ],
      "metadata": {
        "id": "LSfnaPas3JUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use BeautifulSoup to scrape the data from URL"
      ],
      "metadata": {
        "id": "B3SZkHw03ZSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# url to scrape\n",
        "url=\"https://www.scrapethissite.com/pages/forms/\"\n",
        "\n",
        "# a request to get the page\n",
        "page=requests.get(url)\n",
        "\n",
        "# parse the Html content\n",
        "soup=BeautifulSoup(page.content,\"html.parser\")"
      ],
      "metadata": {
        "id": "KLkmPy31ttCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the table on the url and extract the rows and columns"
      ],
      "metadata": {
        "id": "T6CHikEQ4T_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the table\n",
        "table=soup.find(\"table\")\n",
        "\n",
        "# find all rows\n",
        "rows=table.find_all(\"tr\")\n",
        "\n",
        "# extract the columns\n",
        "columns=[v.text.strip() for v in rows[0].find_all(\"th\")]"
      ],
      "metadata": {
        "id": "tPDmjef7veJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(rows)\n",
        "print(columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0oZlf6WwpBR",
        "outputId": "5ac7e6db-2794-4a27-9e46-83037b8039f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Team Name', 'Year', 'Wins', 'Losses', 'OT Losses', 'Win %', 'Goals For (GF)', 'Goals Against (GA)', '+ / -']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the data rows\n",
        "data=[]\n",
        "for row in rows[1:]:\n",
        "    values=[v.text.strip() for v in row.find_all(\"td\")]\n",
        "    data.append(values)"
      ],
      "metadata": {
        "id": "EK9n4upnxmij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a DataFrame using pandas"
      ],
      "metadata": {
        "id": "-ZEqqGBr43yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataframe\n",
        "df=pd.DataFrame(data,columns=columns)\n",
        "\n",
        "# print 5 first rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT8maawIyIWO",
        "outputId": "111213ab-0bf8-433e-ec53-5cd5d863a362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Team Name  Year Wins Losses OT Losses  Win % Goals For (GF)  \\\n",
            "0       Boston Bruins  1990   44     24             0.55            299   \n",
            "1      Buffalo Sabres  1990   31     30            0.388            292   \n",
            "2      Calgary Flames  1990   46     26            0.575            344   \n",
            "3  Chicago Blackhawks  1990   49     23            0.613            284   \n",
            "4   Detroit Red Wings  1990   34     38            0.425            273   \n",
            "\n",
            "  Goals Against (GA) + / -  \n",
            "0                264    35  \n",
            "1                278    14  \n",
            "2                263    81  \n",
            "3                211    73  \n",
            "4                298   -25  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## save the dataset to CSV"
      ],
      "metadata": {
        "id": "VfsXQF-p5cSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"data.csv\",index=False)"
      ],
      "metadata": {
        "id": "jEPNFsBwy5KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Amzon Webscrapping**\n",
        "\n",
        "Categoies:\n",
        "- Electronics\n",
        "- Baby\n",
        "- Shoes\n",
        "- Watches\n",
        "-Cameras"
      ],
      "metadata": {
        "id": "o1OD9SLdx5Ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This Script scrapes images from categories in Amzon website and lables the in to folders, with 5 images per category for all 5 categories\n",
        "'''\n",
        "\n",
        "# Function to save the product image\n",
        "def save_image(image_url, product_name, category_name):\n",
        "    response = requests.get(image_url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img_name = product_name[:20].replace(' ', '_').replace('/', '_') + \".jpg\"\n",
        "\n",
        "    # Ensure the directory exists\n",
        "    directory = os.path.join('images', category_name)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    # Save the image\n",
        "    img_path = os.path.join(directory, img_name)\n",
        "    img.save(img_path)\n",
        "    print(f\"Saved image: {img_path}\")\n",
        "\n",
        "# Function to scrape Amazon product data\n",
        "def scrape_amazon(url, category_name):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "        'Accept-Language': 'en-US,en;q=0.9'\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(f\"Status Code: {response.status_code}\")\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: Unable to access {url}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "    products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "    product_list = []\n",
        "\n",
        "    for product in products[:5]:  # Limit to 5 products\n",
        "        try:\n",
        "            # Extract product name\n",
        "            product_name = product.h2.text.strip()\n",
        "\n",
        "            # Extract product image URL\n",
        "            image_tag = product.find('img', {'class': 's-image'})\n",
        "            image_url = image_tag['src'] if image_tag else None\n",
        "\n",
        "            if image_url:\n",
        "                save_image(image_url, product_name, category_name)\n",
        "\n",
        "            product_list.append({\n",
        "                'name': product_name,\n",
        "                'image_url': image_url,\n",
        "                'category': category_name\n",
        "            })\n",
        "\n",
        "            print(f\"Product: {product_name}\\nImage URL: {image_url}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing product: {e}\")\n",
        "            continue\n",
        "\n",
        "    return product_list\n",
        "\n",
        "# Scrape multiple categories\n",
        "def scrape_amazon_categories():\n",
        "    categories = {\n",
        "        'Baby': 'https://www.amazon.com/s?k=baby',\n",
        "        'Watches': 'https://www.amazon.com/s?k=watches',\n",
        "        'Cameras': 'https://www.amazon.com/s?k=cameras',\n",
        "        'Shoes': 'https://www.amazon.com/s?k=shoes',\n",
        "        'Electronics': 'https://www.amazon.com/s?k=electronics'\n",
        "    }\n",
        "\n",
        "    all_products = []\n",
        "\n",
        "    for category_name, url in categories.items():\n",
        "        print(f\"Scraping category: {category_name}\")\n",
        "        products = scrape_amazon(url, category_name)\n",
        "        if products:\n",
        "            all_products.extend(products)\n",
        "\n",
        "    return all_products\n",
        "\n",
        "# Run the scraper\n",
        "all_products = scrape_amazon_categories()\n",
        "\n",
        "# Display the results\n",
        "print(\"Scraping complete. Products:\")\n",
        "for product in all_products:\n",
        "    print(f\"Category: {product['category']} | Product: {product['name']} | Image URL: {product['image_url']}\")\n"
      ],
      "metadata": {
        "id": "Bztd4lJBLydP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c517f21e-a876-4461-f7d2-4cffbf7149f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping category: Baby\n",
            "Status Code: 200\n",
            "Saved image: images/Baby/Pampers.jpg\n",
            "Product: Pampers\n",
            "Image URL: https://m.media-amazon.com/images/I/61Kcg05Sz4L._AC_UL320_.jpg\n",
            "\n",
            "Saved image: images/Baby/HelloBaby.jpg\n",
            "Product: HelloBaby\n",
            "Image URL: https://m.media-amazon.com/images/I/61WoQMri81L._AC_UL320_.jpg\n",
            "\n",
            "Saved image: images/Baby/The_Honest_Company.jpg\n",
            "Product: The Honest Company\n",
            "Image URL: https://m.media-amazon.com/images/I/81W3iWS7ptL._AC_UL320_.jpg\n",
            "\n",
            "Saved image: images/Baby/Baby_Montessori_Sens.jpg\n",
            "Product: Baby Montessori Sensory Toys for 0-6 6-12 Months, Food Grade Teething Toys for Babies 0 3 6 9 12 18 Months, Newborn Infant Learning Developmental Toys Gifts for 1 2 Year Old Boys Girls\n",
            "Image URL: https://m.media-amazon.com/images/I/61hMVdD84nL._AC_UL320_.jpg\n",
            "\n",
            "Saved image: images/Baby/HUGGIES.jpg\n",
            "Product: HUGGIES\n",
            "Image URL: https://m.media-amazon.com/images/I/71NCICgdEcL._AC_UL320_.jpg\n",
            "\n",
            "Scraping category: Watches\n",
            "Status Code: 503\n",
            "Error: Unable to access https://www.amazon.com/s?k=watches\n",
            "Scraping category: Cameras\n",
            "Status Code: 503\n",
            "Error: Unable to access https://www.amazon.com/s?k=cameras\n",
            "Scraping category: Shoes\n",
            "Status Code: 503\n",
            "Error: Unable to access https://www.amazon.com/s?k=shoes\n",
            "Scraping category: Electronics\n",
            "Status Code: 503\n",
            "Error: Unable to access https://www.amazon.com/s?k=electronics\n",
            "Scraping complete. Products:\n",
            "Category: Baby | Product: Pampers | Image URL: https://m.media-amazon.com/images/I/61Kcg05Sz4L._AC_UL320_.jpg\n",
            "Category: Baby | Product: HelloBaby | Image URL: https://m.media-amazon.com/images/I/61WoQMri81L._AC_UL320_.jpg\n",
            "Category: Baby | Product: The Honest Company | Image URL: https://m.media-amazon.com/images/I/81W3iWS7ptL._AC_UL320_.jpg\n",
            "Category: Baby | Product: Baby Montessori Sensory Toys for 0-6 6-12 Months, Food Grade Teething Toys for Babies 0 3 6 9 12 18 Months, Newborn Infant Learning Developmental Toys Gifts for 1 2 Year Old Boys Girls | Image URL: https://m.media-amazon.com/images/I/61hMVdD84nL._AC_UL320_.jpg\n",
            "Category: Baby | Product: HUGGIES | Image URL: https://m.media-amazon.com/images/I/71NCICgdEcL._AC_UL320_.jpg\n"
          ]
        }
      ]
    }
  ]
}